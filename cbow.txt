import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Sample text data with a single topic for better CBOW prediction
text_data = "Natural language processing and machine learning are advancing artificial intelligence."

# Step 1: Tokenize the text data and get vocabulary size
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text_data])  # Fit on the single sentence
total_words = len(tokenizer.word_index) + 1

# Step 2: Generate context-target pairs for CBOW
def create_context_target_pairs(token_list, window_size):
    input_sequences, labels = [], []
    for i in range(window_size, len(token_list) - window_size):
        context = token_list[i - window_size:i] + token_list[i + 1:i + window_size + 1]
        target = token_list[i]
        input_sequences.append(context)
        labels.append(target)
    return np.array(input_sequences), tf.keras.utils.to_categorical(labels, num_classes=total_words)

# Convert sentence to token indices and create pairs
token_list = tokenizer.texts_to_sequences([text_data])[0]
X, y = create_context_target_pairs(token_list, window_size=2)

# Step 3: Define the CBOW model
def build_cbow_model(vocab_size, embedding_dim):
    model = Sequential([
        Embedding(vocab_size, embedding_dim, input_length=2 * 2),  # 2 * window_size
        GlobalAveragePooling1D(),
        Dropout(0.2),
        Dense(256, activation='relu'),
        Dense(512, activation='relu'),
        Dense(vocab_size, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Create and train the model
model = build_cbow_model(total_words, embedding_dim=10)
model.fit(X, y, epochs=100, verbose=1)

# Step 5: Function to predict a word given its context
def predict_word(context_words):
    context_seq = pad_sequences([tokenizer.texts_to_sequences([context_words])[0]], maxlen=2 * 2, padding='pre')
    predicted_word_index = np.argmax(model.predict(context_seq), axis=-1)[0]
    return tokenizer.index_word.get(predicted_word_index, None)

# Example prediction: input context with two words on each side
context_words = "and machine learning"  # Context around the target word
print("Predicted target word:", predict_word(context_words))
